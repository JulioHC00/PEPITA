{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd06395",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate predictions for the radius ratio precision of TESS Objects of Interest (TOIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a8ac0",
   "metadata": {},
   "source": [
    "\n",
    "## Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba56b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import lightkurve as lk\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "import astropy.constants as c\n",
    "from scipy.constants import G\n",
    "from IPython.display import display\n",
    "from ldtk import LDPSetCreator, BoxcarFilter, TabulatedFilter\n",
    "from exoInfoMatrixTOI import exoInfoMatrix\n",
    "import ldtk.filters as filters\n",
    "import exoplanet as xo\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004912b7",
   "metadata": {},
   "source": [
    "\n",
    "## Select only planet candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc35982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read TOIs table from the Nasa Exoplanet Archive (NEA)\n",
    "nea_tois = pd.read_csv(\"nea_tois.csv\", header=90)\n",
    "\n",
    "nea_tois = nea_tois[nea_tois[\"tfopwg_disp\"] == \"PC\"] # Only want planet candidates\n",
    "nea_tois = nea_tois[nea_tois[\"pl_pnum\"] == 1] # And only with a single planet\n",
    "\n",
    "print(f\"Initial number of planet candidates is {len(nea_tois)}\\n\")\n",
    "\n",
    "# Only keep if there are values for stellar logg and stellar radius which we will need later on\n",
    "nea_tois.dropna(axis=0, subset=[\"st_logg\", \"st_rad\", \"st_teff\"], inplace=True)\n",
    "\n",
    "# We also need errors, wherever two values (lower and upper boundaries) for the error are reported or only one is given, we will keep the largest\n",
    "nea_tois[\"st_rad_err\"] = np.nanmax(nea_tois[[\"st_raderr1\", \"st_raderr2\"]], axis=-1) \n",
    "nea_tois[\"st_logg_err\"] = np.nanmax(nea_tois[[\"st_loggerr1\", \"st_loggerr2\"]], axis=-1) \n",
    "nea_tois[\"st_teff_err\"] = np.nanmax(nea_tois[[\"st_tefferr1\", \"st_tefferr2\"]], axis=-1) \n",
    "\n",
    "# And since we also need the errors later on, only keep columns with errors included\n",
    "nea_tois.dropna(axis=0, subset=[\"st_rad_err\", \"st_logg_err\", \"st_teff_err\"], inplace=True)\n",
    "\n",
    "print(f\"{len(nea_tois)} planet candidates with values for logg, R_star and T_eff along with errors\\n\")\n",
    "\n",
    "# Reset indices\n",
    "nea_tois.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad4801",
   "metadata": {},
   "source": [
    "\n",
    "#### Out of these, we want only those which were observed __only__ with 1800s cadence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d4ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will need to search for the available lightcurves for each of the candidates. Then, if they are only observed with 1800s we add them to a new dataframe\n",
    "\n",
    "# This can take long\n",
    "for i, row in nea_tois.iterrows():\n",
    "    print(f\"\\n{i} out of {len(nea_tois) - 1}\")\n",
    "\n",
    "    TID = f\"TIC {row['tid']}\"\n",
    "\n",
    "    # Results of the lightcurve search\n",
    "    search = lk.search_lightcurve(TID, mission=\"TESS\")\n",
    "\n",
    "    # If there were no matches, a KeyError will be raised\n",
    "    try:\n",
    "        exptimes = set(search.exptime.value)\n",
    "    except KeyError:\n",
    "        # We add a note letting us know this PC wasn't found\n",
    "        nea_tois.at[i, \"notes\"] = \"NOT FOUND\"\n",
    "        print(\"NOT FOUND\")\n",
    "        continue\n",
    "\n",
    "    # We check if the only cadence is 1800s. If it's not we do not flag this candidate as accepted. Otherwise we do\n",
    "    if not exptimes.issubset(set([1800])):\n",
    "        nea_tois.at[i, \"notes\"] = \"NOT ONLY 1800s\"\n",
    "        print(\"NOT ONLY 1800s\")\n",
    "    else:\n",
    "        nea_tois.at[i, \"accepted\"] = True\n",
    "        print(\"ACCEPTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296132d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we save the PCs with observations only in 1800s\n",
    "nea_tois[nea_tois[\"accepted\"] == True].to_csv(\"tois_with_only_1800s.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3db7b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# And read these results into a new dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m only1800 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtois_with_only_1800s.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# And read these results into a new dataframe\n",
    "only1800 = pd.read_csv(\"tois_with_only_1800s.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03c825",
   "metadata": {},
   "source": [
    "\n",
    "### Add columns with some values needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741127a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'only1800' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# We use astropy units to not have to deal with conversion between units.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m logg \u001b[38;5;241m=\u001b[39m \u001b[43monly1800\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mst_logg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# Log(g)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlogg\u001b[38;5;241m.\u001b[39mto_numpy() \u001b[38;5;241m*\u001b[39m u\u001b[38;5;241m.\u001b[39mcm \u001b[38;5;241m*\u001b[39m u\u001b[38;5;241m.\u001b[39ms \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# g in cm/s^2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m R \u001b[38;5;241m=\u001b[39m only1800[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mst_rad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy() \u001b[38;5;241m*\u001b[39m c\u001b[38;5;241m.\u001b[39mR_sun \u001b[38;5;66;03m# Stellar radius in solar radii\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'only1800' is not defined"
     ]
    }
   ],
   "source": [
    "# We use astropy units to not have to deal with conversion between units.\n",
    "\n",
    "\n",
    "logg = only1800[\"st_logg\"] # Log(g)\n",
    "g = 10**logg.to_numpy() * u.cm * u.s ** (-2) # g in cm/s^2\n",
    "R = only1800[\"st_rad\"].to_numpy() * c.R_sun # Stellar radius in solar radii\n",
    "P = only1800[\"pl_orbper\"].to_numpy() * u.day # Period in days\n",
    "T = only1800[\"pl_trandurh\"].to_numpy() * u.hour # Transit duration in hours\n",
    "\n",
    "# We store the values only, no units\n",
    "only1800[\"st_rho\"] = (3/(4 * np.pi * c.G) * g / R).to(u.g * (u.cm)**(-3)).value # Stellar density\n",
    "\n",
    "only1800[\"a\"] = ((g * R ** 2 * P ** 2) / (4 * np.pi ** 2)) ** (1/3) # Semi-major axis\n",
    "\n",
    "only1800.at[1,\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5ddf8",
   "metadata": {},
   "source": [
    "\n",
    "### To make stimates, we need fiducial values for the limb-darkening parameters. We obtain approximate values using 'PyLDTk'.\n",
    "\n",
    "## THIS CAN TAKE LONG AND WILL DOWNLOAD FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d999d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filt = filters.create_tess() # Create TESS filters profiles\n",
    "\n",
    "copy = only1800.copy() # Copy df to iterate through rows safely\n",
    "\n",
    "# Iterate through all rows\n",
    "for i, row in copy.iterrows():\n",
    "    print(f\"Row {i} out of {len(copy) - 1}\")\n",
    "\n",
    "    # Read effective temperature and logg values\n",
    "    teff = row[\"st_teff\"]\n",
    "    teff_err = row[\"st_logg_err\"]\n",
    "\n",
    "    logg = row[\"st_logg\"]\n",
    "    logg_err = row[\"st_logg_err\"]\n",
    "\n",
    "    # Just to be sure, we check there are no NaN values\n",
    "    names = np.array([\"teff\", \"teff_err\", \"logg\", \"logg_err\"])\n",
    "    anynan = np.isnan(np.array([teff, teff_err, logg, logg_err]))\n",
    "\n",
    "    if anynan.any():\n",
    "        print(f\"{row['tid']} has NaN value in {names[anynan]}\")\n",
    "\n",
    "    # Create profiles. Because we have no z value from the table we use 0.25 with error 0.125\n",
    "    sc = LDPSetCreator(teff=(teff, teff_err), logg=(logg, logg_err), z=(0.25, 0.125), filters=[filt])\n",
    "\n",
    "    ps = sc.create_profiles(nsamples=1000)\n",
    "\n",
    "    # Do a mcmc to get the values, if it can't converge print message\n",
    "    try:\n",
    "        qc, qe = ps.coeffs_qd(do_mc=True)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(f\"Row {i} ({row['tid']}) did not converge\")\n",
    "        only1800.at[i, \"u_star1\"] = None\n",
    "        only1800.at[i, \"u_star2\"] = None\n",
    "        only1800.at[i, \"u_star1_sd\"] = None\n",
    "        only1800.at[i, \"u_star2_sd\"] = None\n",
    "        continue\n",
    "\n",
    "    # Check no NaN values in results\n",
    "    if np.isnan([qc,qe]).any():\n",
    "        print(f\"Row {i} ({row['tid']}) calculated values are nan somewhere\")\n",
    "\n",
    "    only1800.at[i, \"u_star1\"] = qc[0][0]\n",
    "    only1800.at[i, \"u_star2\"] = qc[0][1]\n",
    "    only1800.at[i, \"u_star1_sd\"] = qe[0][0]\n",
    "    only1800.at[i, \"u_star2_sd\"] = qe[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# And we save only those that did converge\n",
    "\n",
    "only1800[np.invert(np.isnan(only1800[\"u_star1\"].to_numpy()))].to_csv(\"tois_with_only_1800s_limbdark.csv\", index=False)\n",
    "\n",
    "limbdarkened = pd.read_csv(\"tois_with_only_1800s_limbdark.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830949a",
   "metadata": {},
   "source": [
    "\n",
    "### Now that we have limb-darkening values we can get an approximate value for the radius ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We calculate for each row\n",
    "for i, row in limbdarkened.copy().iterrows():\n",
    "    # We create a limb-darkened star from the exoplanet package\n",
    "    star = xo.LimbDarkLightCurve(row[\"u_star1\"], row[\"u_star2\"])\n",
    "\n",
    "    # And use the 'get_ror_from_approx_transit_depth' utility to obtain an approximate value for the radius ratio\n",
    "    ror = star.get_ror_from_approx_transit_depth(row[\"pl_trandep\"]*1e-6, row[\"b\"]).eval()\n",
    "\n",
    "    limbdarkened.at[i, \"ror\"] = ror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa48191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can now save this as our final dataframe\n",
    "\n",
    "limbdarkened = limbdarkened[np.invert(np.isnan(limbdarkened[\"ror\"]))]\n",
    "\n",
    "print(f\"{len(limbdarkened)} final planet candidates to be passed onto prediction calculation\")\n",
    "\n",
    "limbdarkened.to_csv(\"final_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2eaf6",
   "metadata": {},
   "source": [
    "\n",
    "## Now we make the actual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a550f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To make it faster, we will parallelize the calculations\n",
    "\n",
    "# CHANGE THIS TO THE NUMBER OF CORES YOU WISH TO USE\n",
    "NCORES = 12\n",
    "\n",
    "# Read the final input table\n",
    "table = pd.read_csv(\"final_dataframe.csv\")\n",
    "\n",
    "# Now we split them into NCORES tables\n",
    "tables = np.array_split(table, NCORES)\n",
    "\n",
    "# We will calculate predicted radius ratio for the following exposure times\n",
    "calc_expt = {20, 120, 600, 1800}\n",
    "\n",
    "indices = np.arange(0, NCORES, 1) # Just to keep track of how each thread is doing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931762f",
   "metadata": {},
   "source": [
    "\n",
    "### This function will calculate predictions for each of the tables. We need to include it in a function so as to be able to do multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0deb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_prediction(args):\n",
    "    df, index = args\n",
    "\n",
    "    copy = df.copy()\n",
    "    copy.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Loop through all rows\n",
    "    for idx, row in copy.iterrows():\n",
    "        print(f\"THREAD {index}: {idx+1} out of {len(copy)}\\n\")\n",
    "\n",
    "        # Read the hostname\n",
    "        host = f\"TIC {row['tid']}\"\n",
    "\n",
    "        ref_exptime = 1800 # Our reference exposure time is 1800s, to download a reference lightcurve later one\n",
    "\n",
    "        # Search the lightcurve\n",
    "        search = lk.search_lightcurve(host, mission=\"TESS\", exptime=ref_exptime)\n",
    "\n",
    "        # We give priority to SPOC lightcurves, then QLP and then CDIPS. No reason beyond keeping lightcurves as homogeneous as possible.\n",
    "        if len(search[[\"SPOC\" in author for author in search.author]]) != 0:\n",
    "            search = search[[\"SPOC\" in author for author in search.author]]\n",
    "        elif len(search[[\"QLP\" in author for author in search.author]]) != 0:\n",
    "            search = search[[\"QLP\" in author for author in search.author]]\n",
    "        elif len(search[[\"CDIPS\" in author for author in search.author]]):\n",
    "            search = search[[\"CDIPS\" in author for author in search.author]]\n",
    "\n",
    "        # Download the lightcurve\n",
    "        try:\n",
    "            lc = search[-1].download_all().stitch().remove_nans().remove_outliers(sigma_lower=float('inf'))\n",
    "        except lk.LightkurveError:\n",
    "            print(f\"{host} lightcurve can't be downloaded ({search.author})\")\n",
    "\n",
    "\n",
    "        # Set the reference mean error of measurements as the mean error for the measurements in the 1800s lightcurve\n",
    "        ref_sigma = np.mean(np.array(lc.flux_err.value))\n",
    "\n",
    "        # And the reference timestamps array is also obtained from the lightcurve\n",
    "        ref_t = np.array(lc.time.value)\n",
    "\n",
    "        # We also keep track of these values\n",
    "        copy.at[idx, \"ref_exptime\"] = ref_exptime\n",
    "        copy.at[idx, \"ref_sigma\"] = ref_sigma\n",
    "\n",
    "        # Now we make the actual predictions for each exposure time\n",
    "        for exptime in calc_expt:\n",
    "            # New array of timestamps with points spaced by one exposure time and with a total observation time equal to one sector\n",
    "            t = np.arange(min(ref_t), max(ref_t), exptime / (3600 * 24))\n",
    "\n",
    "            # Calculate the new mean error for this exposure time\n",
    "            sigma = ref_sigma * np.sqrt(ref_exptime)/np.sqrt(exptime)\n",
    "\n",
    "            # Initialize the information matrix. Oversample of ~100 should be fine but can also do 1000, it will just take longer\n",
    "            infomatrix = exoInfoMatrix(exptime, oversample=100)\n",
    "\n",
    "            # This is just to make sure there are no nan values\n",
    "            anynan = np.isnan(np.array([\n",
    "                row[\"pl_orbper\"],\n",
    "                row[\"pl_tranmid\"],\n",
    "                row[\"ror\"],\n",
    "                row[\"b\"],\n",
    "                row[\"u_star1\"],\n",
    "                row[\"u_star2\"],\n",
    "                row[\"st_rho\"],\n",
    "                row[\"st_rad\"]]))\n",
    "\n",
    "            names = np.array([\"pl_orbper\", \"pl_tranmid\", \"ror\", \"b\", \"u_star1\", \"u_star2\", \"st_rho\", \"st_rad\"])\n",
    "\n",
    "            if np.isnan(t).any():\n",
    "                print(f\"{host} has NaN values for t\")\n",
    "                continue\n",
    "            if anynan.any():\n",
    "                print(f\"{host} has NaN values for {names[anynan]}\")\n",
    "                continue\n",
    "\n",
    "            # If there are no NaNs then we set the data\n",
    "            infomatrix.set_data(\n",
    "                time_array = t,\n",
    "                period_val = row[\"pl_orbper\"],\n",
    "                t0_val     = row[\"pl_tranmid\"],\n",
    "                ror_val    = row[\"ror\"],\n",
    "                b_val      = row[\"b\"],\n",
    "                u1_val     = row[\"u_star1\"],\n",
    "                u2_val     = row[\"u_star2\"],\n",
    "                rho_star_val = row[\"st_rho\"],\n",
    "                r_star_val = row[\"st_rad\"],\n",
    "            )\n",
    "\n",
    "            # Then we set the priors. We do not use a prior on stellar density\n",
    "            infomatrix.set_priors(\n",
    "                period_prior = np.nanmax(np.abs(row[[\"pl_orbpererr1\", \"pl_orbpererr2\"]])),\n",
    "                t0_prior = np.nanmax(np.abs(row[[\"pl_tranmiderr1\", \"pl_tranmiderr2\"]])),\n",
    "                r_star_prior = np.nanmax(np.abs(row[[\"st_raderr1\", \"st_raderr2\"]])),\n",
    "                b_prior = 1/np.sqrt(12),\n",
    "                u1_prior = 0.4713,\n",
    "                u2_prior = 0.4084,\n",
    "            )\n",
    "\n",
    "            # And we calculate the information matrix\n",
    "            try:\n",
    "                matrix = infomatrix.eval_cov(sigma = np.mean(sigma))\n",
    "            except ValueError:\n",
    "                print(f\"{host} inversion of matrix failed\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Now we loop through the rows and columns of the matrix to extract the values\n",
    "            for i, value1 in enumerate([\"period\", \"t0\", \"ror\", \"b\", \"u_star1\", \"u_star2\", \"rho_star\", \"r_star\"]):\n",
    "                for j, value2 in enumerate([\"period\", \"t0\", \"ror\", \"b\", \"u_star1\", \"u_star2\", \"rho_star\", \"r_star\"]):\n",
    "\n",
    "                    # Diagonal gives the standard deviation or predicted precision\n",
    "                    if value1 == value2:\n",
    "                        std = np.sqrt(np.abs(matrix[i,j]))\n",
    "                        col = f\"{value1}_{exptime}_sd\"\n",
    "                        copy.at[idx, col] = std\n",
    "\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we parallelize the calculation and execute it\n",
    "# May have problems downloading lightcurves authored by DIAMANTE\n",
    "\n",
    "arguments = [(df, index) for df, index in zip(tables, indices)]\n",
    "\n",
    "p = mp.Pool(NCORES)\n",
    "\n",
    "result = list(p.imap(calculate_prediction, arguments))\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for df in result:\n",
    "    final_df = pd.concat([final_df, df])\n",
    "\n",
    "final_df.to_csv(\"tois_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5fb2c",
   "metadata": {},
   "source": [
    "\n",
    "# Now we calculate the actual improvements in precision by using the predicted precisions and make it into a nice table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c88687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df = pd.read_csv(\"tois_with_predictions.csv\")\n",
    "\n",
    "improvements = pd.DataFrame(columns=['toi', 'tid', '20_improv', '120_improv', 'ror_sd_20', 'ror_sd_120', 'ror_sd_1800'])\n",
    "\n",
    "improvements['toi'] = final_df['toi']\n",
    "improvements['tid'] = final_df['tid']\n",
    "improvements['ror_sd_20'] = final_df['ror_20_sd']\n",
    "improvements['ror_sd_120'] = final_df['ror_120_sd']\n",
    "improvements['ror_sd_600'] = final_df['ror_600_sd']\n",
    "improvements['ror_sd_1800'] = final_df['ror_1800_sd']\n",
    "\n",
    "improvements['20_improv'] = (1 - improvements['ror_sd_20'] / improvements['ror_sd_1800']) * 100\n",
    "improvements['120_improv'] = (1 - improvements['ror_sd_120'] / improvements['ror_sd_1800']) * 100\n",
    "improvements['600_improv'] = (1 - improvements['ror_sd_600'] / improvements['ror_sd_1800']) * 100\n",
    "\n",
    "improvements.sort_values(by=['20_improv'], ascending=False, inplace=True)\n",
    "\n",
    "# And this is our nice table with all predictions\n",
    "improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23f86b",
   "metadata": {},
   "source": [
    "\n",
    "### Can also convert the table to a latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We select the top 10\n",
    "\n",
    "table = improvements.head(10)\n",
    "\n",
    "table.drop(labels=['tid', 'ror_sd_20', 'ror_sd_120', 'ror_sd_600', 'ror_sd_1800'], inplace=True, axis=1)\n",
    "\n",
    "table.rename(columns={\n",
    "    'toi': 'TOI',\n",
    "    '20_improv': '20s Improv. [%]',\n",
    "    '120_improv': '120s Improv [%]',\n",
    "    '600_improv': '600s Improv [%]'\n",
    "}, inplace=True)\n",
    "\n",
    "table.to_latex('improvements_table.tex', index=False, float_format=\"%.2f\")\n",
    "\n",
    "# Which is the table used in the paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
